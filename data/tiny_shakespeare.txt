Q: What is machine learning?
A: Machine learning is a subset of AI where systems learn patterns from data to make predictions or decisions without being explicitly programmed.

Q: What are the three types of machine learning?
A: They are supervised learning, unsupervised learning, and reinforcement learning.

Q: What is supervised learning?
A: Supervised learning uses labeled data to train a model to predict outputs from inputs.

Q: What is unsupervised learning?
A: Unsupervised learning finds hidden patterns and structures in unlabeled data.

Q: What is reinforcement learning?
A: Reinforcement learning trains an agent to take actions to maximize cumulative reward in an environment.

Q: Give an example of supervised learning.
A: Predicting house prices using labeled data containing features and their corresponding prices.

Q: Give an example of unsupervised learning.
A: Grouping customers into clusters based on behavior using K-means.

Q: Give an example of reinforcement learning.
A: Training a robot to walk by rewarding successful steps.

Q: What is a neural network?
A: A neural network is a computational model inspired by the brain, consisting of interconnected layers of nodes (neurons) that learn patterns from data.

Q: What is a perceptron?
A: A perceptron is the simplest neural network unit that performs binary classification by applying weights and a bias to inputs.

Q: What is backpropagation?
A: Backpropagation is the algorithm used to train neural networks by calculating gradients and updating weights using gradient descent.

Q: What is gradient descent?
A: Gradient descent is an optimization algorithm that updates model parameters to minimize the loss by moving in the direction of steepest descent.

Q: What is learning rate?
A: The learning rate controls how much model parameters change during each training step.

Q: What is overfitting?
A: Overfitting occurs when a model memorizes training data and performs poorly on new data.

Q: What is underfitting?
A: Underfitting happens when a model is too simple to learn patterns and performs poorly on both training and testing data.

Q: How to prevent overfitting?
A: Techniques include regularization, dropout, more data, early stopping, and using simpler models.

Q: What is regularization?
A: Regularization adds a penalty to the loss function to prevent large weights and reduce overfitting.

Q: What is L1 regularization?
A: L1 regularization adds the absolute value of weights to the loss, encouraging sparsity.

Q: What is L2 regularization?
A: L2 regularization adds the square of weights to the loss, preventing excessively large weights.

Q: What is dropout?
A: Dropout randomly disables neurons during training to prevent over-reliance on specific nodes.

Q: What is an activation function?
A: An activation function introduces nonlinearity into the network, enabling it to learn complex patterns.

Q: Give examples of activation functions.
A: Examples include ReLU, sigmoid, tanh, and softmax.

Q: What is ReLU?
A: ReLU outputs zero for negative inputs and the input itself if positive.

Q: What is sigmoid?
A: The sigmoid function maps inputs to a value between 0 and 1.

Q: What is softmax?
A: Softmax converts logits to probabilities across multiple classes.

Q: What is loss function?
A: A loss function measures how wrong a model’s predictions are.

Q: Give examples of loss functions.
A: MSE, cross-entropy, and hinge loss.

Q: What is accuracy?
A: Accuracy measures the percentage of correct predictions.

Q: What is precision?
A: Precision is the proportion of true positives among predicted positives.

Q: What is recall?
A: Recall is the proportion of true positives among actual positives.

Q: What is F1-score?
A: F1-score is the harmonic mean of precision and recall.

Q: What is a dataset?
A: A dataset is a collection of data used to train or evaluate a model.

Q: What is training data?
A: Training data is used to train a machine learning model.

Q: What is testing data?
A: Testing data is used to evaluate a trained model’s performance.

Q: What is validation data?
A: Validation data is used to tune hyperparameters during training.

Q: What is a confusion matrix?
A: A confusion matrix is a table that shows true positives, false positives, true negatives, and false negatives.

Q: What is cross-validation?
A: Cross-validation splits data into multiple folds to ensure stable model evaluation.

Q: What is KNN?
A: KNN is a non-parametric algorithm that classifies data based on the majority class of its K nearest neighbors.

Q: What is K-means clustering?
A: K-means clusters data into K groups by minimizing the distance between data points and cluster centers.

Q: What is PCA?
A: PCA is a dimensionality reduction technique that converts correlated variables into uncorrelated principal components.

Q: What is SVM?
A: SVM is a supervised learning algorithm that finds the best hyperplane to separate classes.

Q: What is a decision tree?
A: A decision tree splits data into branches based on conditions to make predictions.

Q: What is random forest?
A: Random forest is an ensemble of multiple decision trees, improving prediction robustness.

Q: What is gradient boosting?
A: Gradient boosting builds models sequentially where each new model corrects previous errors.

Q: What is XGBoost?
A: XGBoost is an efficient, optimized implementation of gradient boosting for performance and speed.

Q: What is deep learning?
A: Deep learning uses neural networks with many layers to learn complex patterns.

Q: What is CNN?
A: CNN is a neural network designed for image-related tasks using convolution layers.

Q: What is RNN?
A: RNN is a neural network designed for sequence data using recurrent connections.

Q: What is LSTM?
A: LSTM is an improved RNN that solves the vanishing gradient problem using gates.

Q: What is GRU?
A: GRU is a simplified LSTM with fewer gates and similar performance.

Q: What is attention mechanism?
A: Attention selects important parts of input data when making predictions.

Q: What is a Transformer?
A: Transformer is a neural network architecture based entirely on attention.

Q: What is GPT?
A: GPT is a transformer-based model for generating text in an autoregressive manner.

Q: What is BERT?
A: BERT is a transformer pre-trained using masked language modeling for understanding text.

Q: What is tokenization?
A: Tokenization splits text into smaller units like words, subwords, or characters.

Q: What is a token?
A: A token is a small unit of text used by language models.

Q: What is embedding?
A: Embedding represents tokens as dense numerical vectors.

Q: What is positional encoding?
A: Positional encoding provides sequence information to Transformers.

Q: What is self-attention?
A: Self-attention computes how much each token should focus on other tokens.

Q: What is multi-head attention?
A: Multi-head attention runs multiple self-attention operations in parallel.

Q: What is beam search?
A: Beam search is a decoding algorithm that keeps multiple candidate sequences.

Q: What is greedy search?
A: Greedy search picks the highest probability token at each step.

Q: What is temperature in text generation?
A: Temperature controls randomness in model outputs.

Q: What is top-k sampling?
A: Top-k sampling limits selection to the top k likely tokens.

Q: What is top-p sampling?
A: Top-p sampling selects tokens from the smallest set whose cumulative probability exceeds p.

Q: What is an epoch?
A: An epoch is one full pass through the training dataset.

Q: What is a batch?
A: A batch is a subset of training data used in one training step.

Q: What is batch size?
A: Batch size is the number of examples processed at once.

Q: What is an optimizer?
A: An optimizer updates model weights during training.

Q: What is Adam optimizer?
A: Adam combines momentum and RMSProp for efficient training.

Q: What is gradient clipping?
A: Gradient clipping limits gradient magnitude to prevent exploding gradients.

Q: What is vanishing gradient?
A: Vanishing gradient occurs when gradients become too small to update weights.

Q: What is exploding gradient?
A: Exploding gradient occurs when gradients become excessively large.

Q: What is a hyperparameter?
A: A hyperparameter controls training settings, not learned from data.

Q: What is model evaluation?
A: Evaluation measures how well a trained model performs on new data.

Q: What is a ROC curve?
A: ROC curve plots true positive rate against false positive rate.

Q: What is AUC?
A: AUC measures area under the ROC curve.

Q: What is normalization?
A: Normalization rescales data to a standard range.

Q: What is standardization?
A: Standardization transforms data to zero mean and unit variance.

Q: What is feature engineering?
A: Feature engineering creates new features to improve model performance.

Q: What is feature scaling?
A: Feature scaling adjusts feature values for better training stability.

Q: What is one-hot encoding?
A: One-hot encoding converts categorical values into binary vectors.

Q: What is the bias–variance tradeoff in machine learning?
A: Bias is the error from overly simple assumptions; variance is the error from sensitivity to fluctuations in training data. Good models aim to minimize both to achieve low total error.

Q: What is regularization and why is it used?
A: Regularization adds a penalty term to the loss function to prevent overfitting. Common methods include L1, L2, and dropout.

Q: What is the difference between L1 and L2 regularization?
A: L1 regularization uses the absolute value of weights and produces sparse models. L2 uses squared weights and reduces weight magnitude smoothly.

Q: What is gradient descent?
A: Gradient descent is an optimization method that updates model parameters by moving in the direction of decreasing loss using the gradient.

Q: What are vanishing and exploding gradients?
A: They occur during backpropagation when gradients become too small or too large, causing training instability, especially in RNNs.

Q: What is batch normalization?
A: Batch normalization normalizes layer inputs to speed training and reduce internal covariate shift.

Q: What is early stopping?
A: Early stopping halts training when validation loss stops decreasing, preventing overfitting.

Q: What is the difference between epoch, batch, and iteration?
A: One epoch is one pass over the dataset. A batch is a subset of data. One iteration is one update of gradients using one batch.

Q: What is the purpose of a learning rate scheduler?
A: It adjusts the learning rate during training to improve convergence.

Q: What is cross-entropy loss?
A: A commonly used loss for classification that measures the difference between predicted probability distribution and true distribution.

Q: What is softmax?
A: Softmax converts raw logits into probabilities that sum to 1.

Q: What is a confusion matrix?
A: A table showing true labels vs predicted labels, used to evaluate classification models.

Q: What is precision, recall, and F1-score?
A: Precision measures correctness of positive predictions. Recall measures ability to find all positives. F1 is the harmonic mean of precision and recall.

Q: What is ROC-AUC?
A: A metric that evaluates the tradeoff between true positive rate and false positive rate.

Q: What is the curse of dimensionality?
A: When data has too many features, models become sparse and distances become meaningless, hurting performance.

Q: What is PCA?
A: Principal Component Analysis reduces dimensionality by projecting data into directions of maximum variance.

Q: What is k-means clustering?
A: An unsupervised algorithm that groups data into k clusters by minimizing within-cluster distances.

Q: What is gradient boosting?
A: A method that builds an ensemble of weak learners sequentially, each correcting previous errors.

Q: What is XGBoost?
A: An optimized gradient boosting framework that supports regularization, parallelization, and efficient tree learning.

Q: What is a decision tree?
A: A tree-structured classifier that splits data based on feature thresholds.

Q: What is a random forest?
A: An ensemble of decision trees trained on bootstrapped datasets with random feature selection.

Q: What is a neural network?
A: A layered model of interconnected neurons that learns from labeled data using backpropagation.

Q: What are activation functions?
A: Functions like ReLU, sigmoid, tanh that add nonlinearity to neural networks.

Q: What is dropout?
A: A regularization technique where random neurons are turned off during training.

Q: What is backpropagation?
A: The algorithm that computes gradients of a neural network using the chain rule.

Q: What is a CNN?
A: Convolutional Neural Network – extracts spatial features using convolution filters and pooling.

Q: What is a filter/kernel in CNNs?
A: A small matrix that slides over images to detect patterns like edges and textures.

Q: What is pooling?
A: A downsampling operation that reduces spatial size, e.g., max-pooling.

Q: What is an RNN?
A: Recurrent Neural Network – processes sequences by maintaining state across time steps.

Q: What is LSTM?
A: Long Short-Term Memory – an RNN variant with gates to avoid vanishing gradients.

Q: What is GRU?
A: Gated Recurrent Unit – similar to LSTM but with fewer parameters.

Q: What is a transformer model?
A: A deep learning architecture using self-attention mechanisms, replacing RNNs in NLP tasks.

Q: What is attention?
A: A mechanism that lets models focus on specific parts of input when generating outputs.

Q: What is multi-head attention?
A: Multiple attention layers running parallel to capture different patterns.

Q: What is positional encoding?
A: Adding position information to token embeddings in transformers.

Q: What is fine-tuning?
A: Training an already pre-trained model on a smaller dataset for a specific task.

Q: What is transfer learning?
A: Using knowledge learned in one task/model to improve performance in another.

Q: What is overfitting?
A: When a model memorizes training data but performs poorly on new data.

Q: What is underfitting?
A: When a model is too simple to learn patterns in the data.

Q: What is a hyperparameter?
A: Settings like learning rate, number of layers, batch size, chosen before training.

Q: What is model checkpointing?
A: Saving model weights during training for recovery or later use.

Q: What is quantization?
A: Reducing precision of model weights (e.g., FP32 → INT8) to make models smaller and faster.

Q: What is pruning?
A: Removing unimportant weights or neurons to compress the model.

Q: What is knowledge distillation?
A: A smaller model (student) learns from a larger model (teacher).

Q: What is a loss landscape?
A: A 3D view of how model parameters influence loss.

Q: What is gradient clipping?
A: Limiting gradient magnitude to avoid exploding gradients.

Q: What is teacher forcing in RNNs?
A: Using the true previous token instead of the predicted token during training.

Q: What is beam search?
A: A decoding strategy that keeps multiple best sequences during text generation.

Q: What is perplexity?
A: A metric for language models; lower perplexity = better predictions.

Q: What is cosine similarity?
A: A metric to measure similarity between two vectors using angle difference.

Q: What is an embedding?
A: A dense vector representation of text, images, or categories.

Q: What is a learning rate?
A: A hyperparameter controlling step size during gradient descent.

Q: What is tokenization in NLP?
A: Tokenization is the process of splitting raw text into smaller units such as words, subwords, or characters. Modern transformer models use subword tokenization like Byte-Pair Encoding (BPE) or WordPiece to handle unknown words efficiently.

Q: What is a subword tokenizer and why is it better than word-level tokenization?
A: Subword tokenizers break rare words into smaller units so the model can represent any word. They reduce vocabulary size and handle unseen words without producing UNK tokens.

Q: What is a Transformer model?
A: A Transformer is a neural network architecture based on self-attention. It processes all tokens in parallel instead of sequentially, allowing large-scale training with better long-range context understanding.

Q: What is Attention?
A: Attention is a mechanism that lets the model focus on relevant parts of the input. It calculates similarity between tokens to understand relationships.

Q: What is Self-Attention?
A: Self-attention computes attention weights among all tokens within the same sequence. It helps the model capture context, dependencies, and relationships completely in parallel.

Q: What is Multi-Head Attention?
A: Multi-head attention runs multiple self-attention mechanisms in parallel. Each head learns different types of relationships (syntax, semantics, long-range dependencies).

Q: What is the difference between Encoder and Decoder in a Transformer?
A: Encoder processes input tokens and builds contextual representations. Decoder generates output tokens step-by-step using encoder outputs and its own past predictions with masked attention.

Q: What is Masked Self-Attention?
A: Masked attention prevents the model from seeing future tokens during training. Required for autoregressive generation (GPT-like models).

Q: What is Positional Encoding?
A: Transformers have no recurrence, so positional encodings inject information about token order using sin/cos functions or learned embeddings.

Q: What is an Autoregressive Language Model?
A: An autoregressive LM predicts the next token based on previous tokens. Examples: GPT, LLaMA, Mistral.

Q: What is a Seq2Seq model?
A: Sequence-to-Sequence models convert an input sequence to an output sequence. Example: translation (English → French). Usually uses encoder-decoder architecture.

Q: What is Cross-Attention?
A: Cross-attention lets the decoder attend to encoder outputs. It is used in translation and text-to-text models like T5.

Q: What is a Context Window?
A: Context window is the number of tokens a model can see at once. Larger windows allow better long-document understanding.

Q: What is the difference between BERT and GPT?
A: BERT is a bidirectional encoder (masked-language modeling) for understanding tasks. GPT is a decoder-only autoregressive model for text generation.

Q: What is Masked Language Modeling (MLM)?
A: MLM predicts randomly masked tokens. Used by BERT-like models to learn bidirectional context.

Q: What is Next Token Prediction?
A: NTP predicts the next word in a sequence. Used by GPT models for generation-focused training.

Q: What is Fine-Tuning in NLP?
A: Fine-tuning trains a pre-trained model on a specific labeled dataset, adapting it to tasks like sentiment analysis or question answering.

Q: What is LoRA?
A: LoRA (Low-Rank Adaptation) reduces fine-tuning cost by injecting small trainable matrices into attention layers instead of updating all parameters.

Q: What is Quantization?
A: Quantization reduces model weights from 32-bit to lower formats (4-bit, 8-bit) making inference faster and more memory efficient.

Q: What is Distillation?
A: Distillation trains a small student model to mimic a large teacher model, keeping most performance with fewer parameters.

Q: What is Perplexity?
A: Perplexity measures how well a language model predicts future tokens. Lower perplexity means better language modeling quality.

Q: What is Beam Search?
A: Beam search keeps multiple best candidate sequences during generation instead of greedy decoding. Produces higher-quality outputs.

Q: What is Top-k Sampling?
A: Top-k restricts sampling to the top k most probable tokens. Helps improve randomness while avoiding extremely unlikely tokens.

Q: What is Top-p (Nucleus) Sampling?
A: Top-p sampling selects tokens until cumulative probability reaches p (e.g., 0.9). More adaptive and natural than top-k.

Q: What is Embedding Dimension?
A: Embedding dimension is the vector size used to represent each token. Larger dimensions encode richer semantics.

Q: What is Layer Normalization?
A: LayerNorm stabilizes training by normalizing activations across features. Used in almost every transformer block.

Q: What is Feed Forward Network (FFN) in Transformers?
A: FFN is a 2-layer fully-connected network inside each transformer block that processes each token independently after attention.

Q: What is Training Loss in LLMs?
A: Most LLMs use cross-entropy loss between predicted tokens and ground-truth tokens to measure performance.

Q: What is an Embedding Matrix?
A: It is a lookup table mapping token IDs to dense vectors. The first layer of all language models.

Q: What is the difference between BPE and WordPiece?
A: Both are subword algorithms. BPE merges frequent pairs of characters; WordPiece selects merges based on maximizing likelihood.

Q: What is Temperature in text generation?
A: Temperature controls randomness. Higher temperature = more creativity, lower = more deterministic.

Q: What is a Causal Transformer?
A: A causal transformer uses masked attention so each token only attends to previous tokens. Used in GPT-style models.

Q: What is a Bidirectional Transformer?
A: A bidirectional transformer allows tokens to attend left and right (full context). Used in BERT-style models.

Q: What is T5?
A: T5 is an encoder-decoder transformer trained with text-to-text format, where every task is converted to text generation.

Q: What is the difference between training and inference in NLP models?
A: Training updates model weights using data. Inference runs the model with fixed parameters to generate or classify text.

Q: What is Contextual Embedding?
A: Contextual embeddings depend on the full sentence context instead of static word meanings. Example: BERT & GPT embeddings.

Q: What is Encoder-Only vs Decoder-Only vs Encoder-Decoder?
A: Encoder-only (BERT): understanding tasks  
Decoder-only (GPT): generation tasks  
Encoder-Decoder (T5, LLaMA-D): translation & text-to-text tasks

Q: What causes Hallucinations in LLMs?
A: Limited training data, overconfidence, ambiguous queries, and probability-based generation lead models to produce false facts.

Q: What is a Semantic Vector?
A: A dense numerical vector representing meaning of text. Used in search, embeddings, and retrieval-augmented generation.

Q: What is the difference between word-level, subword-level, and character-level tokenization?
A: Word-level tokenization splits text based on spaces but suffers from out-of-vocabulary words. Character-level tokenization uses individual characters and has no OOV issues but produces long sequences. Subword-level tokenization (BPE, WordPiece, SentencePiece) strikes a balance by breaking rare words into smaller units, reducing vocabulary size and handling unknown words efficiently.

Q: What is Byte Pair Encoding (BPE)?
A: BPE is a subword tokenization method that starts from characters and iteratively merges the most frequent character pairs into new tokens. It helps models represent rare and unseen words while keeping vocabulary compact.

Q: Why do Transformers use positional embeddings?
A: Transformers process tokens in parallel, not sequentially like RNNs. Positional embeddings provide information about the order of tokens so that the model can understand sentence structure and relationships.

Q: What is self-attention?
A: Self-attention allows each token to attend to every other token in a sequence. It calculates similarity scores between all token pairs using queries, keys, and values, enabling the model to capture long-range dependencies efficiently.

Q: What is multi-head attention?
A: Multi-head attention uses multiple attention heads to learn different types of relationships between tokens. Each head captures unique patterns, and their outputs are combined to produce richer contextual representation.

Q: What is the purpose of LayerNorm in Transformers?
A: LayerNorm stabilizes and normalizes activations across features before they pass through attention or feed-forward networks. It improves training stability and prevents exploding or vanishing gradients.

Q: What is the Feed-Forward Network (FFN) in a Transformer block?
A: The FFN is a two-layer fully connected network applied to each token independently. It expands hidden dimensions (e.g., 2048 → 512) and applies nonlinearity (GELU/ReLU) to improve representation power.

Q: What is the difference between encoder-only, decoder-only, and encoder-decoder models?
A: Encoder-only models (e.g., BERT) are bidirectional and good for understanding tasks. Decoder-only models (e.g., GPT) are autoregressive and good for text generation. Encoder-decoder models (e.g., T5, BART) handle translation and sequence-to-sequence tasks.

Q: What is attention masking?
A: Attention masking disables attention to specific tokens. Causal masks prevent the model from attending to future tokens. Padding masks prevent attention to padded positions in variable-length sequences.

Q: What is the purpose of the softmax in attention?
A: Softmax converts unnormalized attention scores into probabilities, ensuring the weights sum to 1. This helps the model focus more on important tokens and less on irrelevant ones.

Q: What is the Transformer’s computational complexity?
A: The full self-attention mechanism has O(n²) time and memory complexity because attention must compute pairwise interactions between all tokens.

Q: How do models reduce Transformer attention cost?
A: Through efficient architectures like Linformer, Performer, Longformer, FlashAttention, and Sparse Attention. These reduce O(n²) to near O(n) using approximations or windowed attention.

Q: What is token embedding vs positional embedding?
A: Token embeddings map each token ID to a dense vector. Positional embeddings encode the token’s location in the sequence. They are added together to form the final input embeddings.

Q: What is prompt tuning?
A: Prompt tuning learns a small set of trainable vectors (soft prompts) that steer a frozen LLM to perform a task without modifying the model weights.

Q: What is fine-tuning vs LoRA?
A: Full fine-tuning updates all model parameters, requiring massive compute. LoRA updates only low-rank matrices inserted inside attention layers, drastically reducing training cost and memory usage.

Q: What is masked language modeling?
A: In MLM, some tokens are masked and the model must predict them. It is used by BERT to learn contextual bidirectional representations.

Q: What is causal language modeling?
A: In CLM, the model predicts the next token based on previous tokens (left-to-right). This is used by GPT to generate text autoregressively.

Q: What is cross-attention?
A: Cross-attention allows the decoder to attend to encoder outputs. It is crucial in sequence-to-sequence models for tasks like translation or summarization.

Q: Why do Transformers outperform RNNs?
A: Transformers parallelize computation, capture long-range dependencies better, avoid vanishing gradients, and scale efficiently with data and model size.

Q: What is perplexity in language models?
A: Perplexity measures how well a model predicts text. Lower perplexity indicates better performance. It is computed as the exponentiated average negative log likelihood over tokens.

Q: What are embeddings?
A: Embeddings are dense vector representations of tokens that capture semantic meaning. Similar words have similar embeddings in vector space.

Q: What is attention head redundancy?
A: Many attention heads learn duplicate or similar patterns. Pruning redundant heads can reduce model size with minimal accuracy loss.

Q: How does a Transformer generate text during inference?
A: The model predicts the next token, appends it to the sequence, and repeats. Decoding strategies include greedy decoding, top-k sampling, top-p (nucleus) sampling, and beam search.

Q: What is temperature in text generation?
A: Temperature controls randomness. A lower temperature makes outputs deterministic, while a higher temperature increases diversity and creativity.

Q: What is positional encoding using sine and cosine?
A: Transformers use sinusoidal functions to encode positions with unique patterns. These encodings allow the model to generalize to sequence lengths not seen during training.

Q: What is gradient checkpointing?
A: Gradient checkpointing saves memory by storing fewer intermediate activations. It recomputes some during backpropagation, allowing training larger models on limited GPUs.

Q: What is FlashAttention?
A: FlashAttention is a memory-efficient attention algorithm that computes exact attention using tiling. It drastically reduces memory consumption and speeds up training.

Q: What is quantization in LLMs?
A: Quantization reduces model weights from FP16/FP32 to lower bit precision (8-bit, 4-bit). It reduces memory, improves inference speed, and enables running LLMs on smaller GPUs.

Q: What is the difference between fine-tuning and RAG?
A: Fine-tuning modifies model weights to store new knowledge. RAG retrieves external documents and uses them at runtime. RAG avoids catastrophic forgetting and reduces training cost.

Q: What is catastrophic forgetting?
A: When a model forgets previously learned knowledge while fine-tuning on new data. Techniques like low-rank adapters, regularization, and replay buffers help mitigate this.

Q: What is a tokenizer vocabulary?
A: It is the set of all tokens (words, subwords, symbols) that the tokenizer can produce. Each token is mapped to an ID used by the model.

Q: Why are transformer blocks stacked?
A: Stacking layers increases representation depth, enabling the model to learn hierarchical and complex patterns across the sequence.

Q: What is head dimension in attention?
A: In multi-head attention, the embedding dimension is split across multiple heads. Each head learns different representations in smaller subspaces, improving expressiveness.

