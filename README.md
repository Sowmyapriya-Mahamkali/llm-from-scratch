<h1 align="center">LLM From Scratch</h1>

<p align="center">
  <b>Mini-GPT Style Transformer | PyTorch | Custom Tokenizer | FastAPI Web UI</b>
</p>

<p align="center">
  <b>Developed by:</b> <a href="#">M. Sowmyapriya</a> & <a href="#">Lalitha</a>
</p>

<p align="center">
  <a href="#project-overview">Overview</a> ‚Ä¢
  <a href="#features">Features</a> ‚Ä¢
  <a href="#tech-stack">Tech Stack</a> ‚Ä¢
  <a href="#quick-start">Quick Start</a> ‚Ä¢
  <a href="#impact">Impact</a>
</p>

---

## üöÄ Project Overview
**LLM From Scratch** is a **miniature GPT-style language model** built entirely from scratch in **PyTorch**.  
It is designed for **learning, experimentation, and research**, showcasing a full **tokenization ‚Üí training ‚Üí inference pipeline**.  

This project demonstrates the capability to design **custom language models**, implement **transformer architectures**, and integrate them with a **FastAPI backend** and **browser-based web interface**.

> ‚ö†Ô∏è **Note:** Large checkpoints (~4GB) are excluded from this repo. Download separately from [Google Drive](#) and place in `checkpoints/`.

---

## üåü Key Features

| Feature | Description |
|---------|-------------|
| **Custom Tokenizer** | BPE tokenizer with optional HuggingFace integration |
| **Transformer Architecture** | Multi-head causal attention, feed-forward layers, layer normalization |
| **Text Generation** | Top-k and nucleus (top-p) sampling for high-quality outputs |
| **Training Pipeline** | Mixed precision, checkpointing, learning rate scheduling |
| **Web UI** | Browser-based interactive chat with FastAPI backend |
| **Lightweight & Modular** | Clear project structure for experimentation and learning |

---

## üóÇ Repository Structure

**llm_project/** ‚Äì Root folder for the LLM project  
‚îú‚îÄ **README.md** ‚Äì Project overview and instructions  
‚îú‚îÄ **config.yaml** ‚Äì Training and model configuration parameters  
‚îú‚îÄ **requirements.txt** ‚Äì Python dependencies  
‚îú‚îÄ **run_llm.py** ‚Äì Script to run training or inference  
‚îú‚îÄ **data/** ‚Äì Sample datasets (e.g., `tiny_shakespeare.txt`)  
‚îú‚îÄ **training/** ‚Äì Training scripts, checkpoints, and utilities  
‚îú‚îÄ **model/** ‚Äì Transformer model implementation (attention, feed-forward layers, etc.)  
‚îú‚îÄ **tokenizer/** ‚Äì Tokenizer scripts and vocabulary files  
‚îú‚îÄ **inference/** ‚Äì Scripts for generating text using the trained model  
‚îú‚îÄ **notebooks/** ‚Äì Experiment notes and analysis in Markdown/Jupyter notebooks  
‚îî‚îÄ **web_ui/** ‚Äì Browser-based web interface (FastAPI backend + HTML/JS frontend)  

> ‚ö†Ô∏è **Note:** Large model checkpoints (~4+ GB) are excluded from this repo. Download separately from [Google Drive](#) and place in `checkpoints/` before running inference.


---

## üéØ Features

- **Tokenizer:** Custom BPE, HuggingFace-compatible  
- **Transformer Model:** Multi-head attention, feed-forward layers, layer normalization  
- **Text Generation:** Supports top-k and top-p sampling  
- **Training:** Mixed precision, learning rate scheduling, checkpointing  
- **Web Interface:** Interactive browser-based chat connected to FastAPI  

---

## üõ† Tech Stack

**Languages:** Python, JavaScript  
**AI/ML:** PyTorch, Transformers, Tokenizers, Whisper  
**Web/API:** FastAPI, HTML/JS  
**Tools:** Jupyter Notebook, Git, GitHub  
**Data Storage:** Local datasets + external checkpoints  

---

## üöÄ Quick Start

# 1. Create & activate virtual environment
python -m venv venv
# Windows
venv\Scripts\activate
# Linux/macOS
source venv/bin/activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Prepare dataset
# Place training file at data/tiny_shakespeare.txt

# 4. Train the model
python training/trainer.py --config config.yaml

# 5. Start FastAPI server
python api/server.py

# 6. Launch web UI
# Open web_ui/static/index.html in your browser

üí° Why This Project Matters

Demonstrates end-to-end LLM pipeline from scratch
Provides hands-on experience with Transformer internals
Ideal for learning, research, and portfolio showcase

üåê Connect With Authors

M. Sowmyapriya: Coming Soon| sowmyapriya7325@gmail.com

Lalitha: Coming Soon | lalitha.koruprolu29@gmail.com
